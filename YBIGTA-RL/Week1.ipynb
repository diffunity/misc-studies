{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Introduction\n",
    "> Materials from [David Silver](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf) | [OpenAI spinning up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "\n",
    "Interaction between an agent and an environment\n",
    "\n",
    "At each time step $t$, \n",
    "1. **agent**\n",
    "    * executes action $A_t$\n",
    "    * Receives observation $O_t$\n",
    "    * Receives scalar reward $R_t$\n",
    "2. **environment**\n",
    "    * Receives action $A_t$\n",
    "    * Emits observation $O_{t+1}$\n",
    "    * Emits scalar reward $R_{t+1}$\n",
    "    \n",
    "\n",
    "* #### Agent\n",
    "The computer software that carries out an action according to a given environment. Goal of reinforcement learning is to optimize the actions of this agent in order to get the most favorable outcomes from the given environment.\n",
    "\n",
    "* #### States \n",
    "A state $s$ is a complete description of the state of the world. There is no information about the world which is hidden from the state. \\\n",
    "Representation: real-valued vector, matrix, or higher-order tensor \\\n",
    "$$ s_{t+1} = f(s_t, a_t) $$\n",
    "\n",
    "* #### Observations\n",
    "An observation $o$ is a partial description of a state that the agent gets to \"see\" (fully observed or partially observed) \\\n",
    "Representation: real-valued vector, matrix, or higher-order tensor\n",
    "\n",
    "* #### Action spaces\n",
    "The set of all valid actions in a given environment. \\\n",
    "i) Discrete action spaces (eg. left, right, stop, up, down, etc.) \\\n",
    "ii) Continuous action spaces (eg. robot control, angles, velocities, etc.)\n",
    "\n",
    "* #### Policies\n",
    "A policy is a rule used by an agent to decide what actions to take. (This is basically the agent) \n",
    "> Deterministic Policy: $$ a_t = \\mu_{\\theta}(s_t) $$\n",
    "> Stochastic Policy: $$ a_t \\sim \\pi_{\\theta}(\\cdot | s_t)$$\n",
    "\n",
    "\n",
    "* #### Trajectories (or history)\n",
    "> Trajectories: \n",
    "$$ \\tau = (s_0, a_0, s_1, a_1, ...) $$\n",
    "$$ s_{t+1} = f(s_t, a_t) $$\n",
    "> History: \n",
    "$$ H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t $$\n",
    "$$ S_t = f(H_t) $$\n",
    "\n",
    "* #### Returns\n",
    "Infinite-horizon discounted return \\\n",
    "for  $\\gamma \\in (0,1)$,\n",
    "$$R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t$$\n",
    "at $\\gamma = 1$, it can be expressed as finite-horizon undiscounted return over some pre-defined time window $T$\n",
    "$$R(\\tau) = \\sum_{t=0}^{T} r_t$$\n",
    "* #### Model\n",
    "A model predicts what the environment will do next.\n",
    "(in model-free environments, we do not use this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Functions\n",
    "\n",
    "The **On-Policy Value Function**, $V^{\\pi}(s)$, which gives the expected return if you start in state s and always act according to policy $\\pi$:\n",
    "\n",
    "$$ V^{\\pi}(s) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi} [{R(\\tau)| s_0 = s}] $$\n",
    "\n",
    "The **On-Policy Action-Value Function**, $Q^{\\pi}(s,a)$, which gives the expected return if you start in state $s$, take an arbitrary action $a$ (which may not have come from the policy), and then forever after act according to policy $\\pi$:\n",
    "\n",
    "$$Q^{\\pi}(s,a) = \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi}[{R(\\tau)| s_0 = s, a_0 = a}]$$\n",
    "\n",
    "The **Optimal Value Function**, $V^*(s)$, which gives the expected return if you start in state $s$ and always act according to the optimal policy in the environment:\n",
    "\n",
    "$$V^*(s) = \\max_{\\pi} \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi}[{R(\\tau)| s_0 = s}]$$\n",
    "$$ V^*(s) = \\max_{\\pi} V^{\\pi}(s)$$ \n",
    "\n",
    "The **Optimal Action-Value Function**, $Q^*(s,a)$, which gives the expected return if you start in state $s$, take an arbitrary action $a$, and then forever after act according to the optimal policy in the environment:\n",
    "\n",
    "$$Q^*(s,a) = \\max_{\\pi} \\mathop{\\mathbb{E}}_{\\tau \\sim \\pi}[{R(\\tau)| s_0 = s, a_0 = a}]$$\n",
    "$$Q^*(s,a) = \\max_{\\pi} Q^{\\pi}(s,a) $$\n",
    "\n",
    "The optimal value functions tell you how you can get the best results with a certain optimized policy. Therefore, if this function can be defined, then the problem of optimal policy is solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation\n",
    "\n",
    "Used for arriving at the value function \\\n",
    "Decompose the value function into two parts. \\\n",
    "i) immediate reward $ R_{t} $ \\\n",
    "ii) discounted value of successor rate $\\gamma v(S_{t})$ \n",
    "\n",
    "* Bellman Equation\n",
    "\n",
    "\n",
    "$$ v(s) = E[G_t | S_t = s] $$\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathop{\\mathbb{E}}_{a \\sim \\pi \\\\ s'\\sim P }[r(s,a) + \\gamma V^{\\pi}(s')] $$\n",
    "\n",
    "$$Q^{\\pi}(s,a) = \\mathop{\\mathbb{E}}_{s'\\sim P}[{r(s,a) + \\gamma \\mathop{\\mathbb{E}}_{a'\\sim \\pi}[{Q^{\\pi}(s',a')}}]] $$\n",
    "\n",
    "Bellman Equations are linear, and hence can be solved directly as long as we have the parameters\n",
    "\n",
    "* Bellman Optimality Equation\n",
    "\n",
    "$$ v^{*}(s) = \\max_{a} q^{*}(s,a) $$\n",
    "\n",
    "$$V^*(s) = \\max_a \\mathop{\\mathbb{E}}_{s'\\sim P}[{r(s,a) + \\gamma V^*(s')}]$$\n",
    "$$Q^*(s,a) = \\mathop{\\mathbb{E}}_{s'\\sim P}[{r(s,a) + \\gamma \\max_{a'} Q^*(s',a')}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a partial ordering over policies\n",
    "$$ \\pi \\geq \\pi^{'}  \\text{  if  }  V^{\\pi}(s) \\geq V^{\\pi^{'}}(s) \\text{ ,  } \\forall s $$\n",
    "\n",
    "For any MDP,\n",
    "$$ \\exists \\pi^{*} \\geq \\pi \\text{ ,  } \\forall s $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization Problem**\n",
    "$$\\pi^{∗}(s) = \\arg \\max_{a} Q^{∗}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Q-Learning Implementation\n",
    "\n",
    "Q-learning Optimization (among many...)\n",
    "\n",
    "$$ Q^{new}(s_{t}, a_{t}) \\leftarrow (1-\\alpha) Q(s_{t},a_{t}) + \\alpha \\cdot ({r_{t} + \\gamma \\cdot \\max_{a} Q(s_{t+1},a)})$$\n",
    "\n",
    "where $\\alpha = $ learning rate, and $\\gamma = $ discount factor\n",
    "\n",
    "> optimization equation from [here](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)\n",
    "\n",
    "#### Environment Description\n",
    "\n",
    "A binary tree with tree_n number of nodes.\n",
    "\n",
    "Agent starts at the root node and traverses the binary tree downwards until it reaches the leaves.\n",
    "\n",
    "Agent chooses to go either left or right at each node.\n",
    "\n",
    "Going left gives a reward of 5; going right gives a reward of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space : discrete [0, 1]\n",
      "Observation Space : discrete [1, ... , 1000]\n"
     ]
    }
   ],
   "source": [
    "action_space = [0,1]\n",
    "observation_space = [i+1 for i in range(tree_n)]\n",
    "q_table = np.zeros((len(observation_space), len(action_space)))\n",
    "\n",
    "print(f\"Action Space : discrete {action_space}\")\n",
    "print(f\"Observation Space : discrete [{min(observation_space)}, ... , {max(observation_space)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment(state, action):\n",
    "    if state >= tree_n/2:\n",
    "        return 1, 0, True\n",
    "    else:\n",
    "        return state * 2 + (not action), [10,5][action], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "current_state = random.sample(observation_space, 1)[0]\n",
    "alpha = 0.3\n",
    "gamma = 0.02\n",
    "for epochs in range(100000):\n",
    "    random_action = random.sample(action_space, 1)[0]\n",
    "    next_state, reward, done = environment(current_state, random_action)\n",
    "    \n",
    "    current_reward = q_table[current_state-1,random_action]\n",
    "    next_best = np.max(q_table[next_state-1])\n",
    "    q_table[current_state-1,random_action] = \\\n",
    "        (1 - alpha) * current_reward + (alpha) * (reward + gamma * next_best)\n",
    "    current_state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State : 1\n",
      "Decision: LEFT\n",
      "Current State : 3\n",
      "Decision: LEFT\n",
      "Current State : 7\n",
      "Decision: LEFT\n",
      "Current State : 15\n",
      "Decision: LEFT\n",
      "Current State : 31\n",
      "Decision: LEFT\n",
      "Current State : 63\n",
      "Decision: LEFT\n",
      "Current State : 127\n",
      "Decision: LEFT\n",
      "Current State : 255\n",
      "Decision: LEFT\n",
      "Current State : 511\n",
      "Decision: RIGHT\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "current_state = random.sample(observation_space[:10], 1)[0]\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "decision_dict = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "while not done:\n",
    "    decision = np.argmax(q_table[current_state-1])\n",
    "    \n",
    "    print(f\"Current State : {current_state}\")\n",
    "    print(f\"Decision: {decision_dict[decision]}\")\n",
    "    \n",
    "    next_state, reward, done = environment(current_state, decision)\n",
    "    \n",
    "    total_reward += reward\n",
    "    current_state = next_state\n",
    "        \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the rows in the q_table belonging to the non-leaf node states, values for going left are larger than going right.\\\n",
    "q_table has properly learned that going left gives better rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.2040816 ,  5.2040816 ],\n",
       "       [10.20008908,  5.20008343],\n",
       "       [10.20408004,  5.20408003],\n",
       "       [10.00410852,  5.00411394],\n",
       "       [10.00410773,  5.00419827],\n",
       "       [10.00410216,  5.00435844],\n",
       "       [10.20408004,  5.20408003],\n",
       "       [10.00424575,  5.0041001 ],\n",
       "       [10.2040816 ,  5.2040816 ],\n",
       "       [10.20408004,  5.20408003]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[random.sample(range(tree_n//2),10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, values for the leaf node states do not vary much. This is since the agent gets no reward at the leaf nodes, and hence, going left or right returns similar rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24561288, 0.20414914],\n",
       "       [0.20744186, 0.20488638],\n",
       "       [0.20495354, 0.20463866],\n",
       "       [0.20417808, 0.20525479],\n",
       "       [0.20524934, 0.20489505],\n",
       "       [0.20881626, 0.20574167],\n",
       "       [0.20489505, 0.20427913],\n",
       "       [0.20409759, 0.20574978],\n",
       "       [0.20465648, 0.21093924],\n",
       "       [0.204362  , 0.20525601]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[random.sample(range(tree_n//2,tree_n),10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI gym example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2) \n",
      "[ 0  1 ]\n",
      "Action Space example :0\n",
      "Observation Space: Box(4,) \n",
      " [ (-4.8, 4.8) \n",
      " (-3.4028235e+38, 3.4028235e+38) \n",
      " (-0.41887903, 0.41887903) \n",
      " (-3.4028235e+38, 3.4028235e+38) ]\n",
      "Observation Space example: [-3.67602158e+00 -2.60373527e+38 -1.05218515e-01  1.67419020e+38]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action Space: {env.action_space} \\n\" + \"[\" + \\\n",
    "      (\" {} \"*env.action_space.n).format(*range(env.action_space.n)) + \"]\")\n",
    "\n",
    "print(f\"Action Space example :{env.action_space.sample()}\")\n",
    "\n",
    "print(f\"Observation Space: {env.observation_space} \\n [\"+(\" {} \\n\"*4).format(\n",
    "    *zip(env.observation_space.low, env.observation_space.high))[:-1]+\"]\")\n",
    "\n",
    "print(f\"Observation Space example: {env.observation_space.sample()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://gym.openai.com/\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
