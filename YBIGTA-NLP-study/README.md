# ybigta_nlp_study
YBIGTA 사이언스 팀 자연어처리 스터디

논문
* [Efficient Representation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
* [Ditributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) 
* [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364)
* [Adversarial Multi-task Learning for Text Classification](https://arxiv.org/abs/1704.05742) 
* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
* [Text Understanding with the Attention Sum Reader Network](https://arxiv.org/abs/1603.01547) 
* [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
* [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365)
* [Improving Language Understanding by Generative Pre-Training](https://pdfs.semanticscholar.org/cd18/800a0fe0b668a1cc19f2ec95b5003d0a5035.pdf?_ga=2.40131109.647956165.1588658157-925505706.1588658157)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)


스터디 원
* 김승유 (YBIGTA 15기)
* 김지수 (YBIGTA 16기)
* 노시영 (YBIGTA 16기)
* 안유선 (YBIGTA 16기)
* 윤민주 (YBIGTA 16기)
* 정정호 (YBIGTA 16기, 스터디장)
