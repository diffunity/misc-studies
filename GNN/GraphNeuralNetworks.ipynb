{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks\n",
    "\n",
    "[Microsoft Lecture Part 1](https://www.youtube.com/watch?v=zCEYiCxrL_0)\n",
    "\n",
    "[Microsoft Lecture Part 1](https://www.youtube.com/watch?v=cWIeTMklzNg)\n",
    "\n",
    "[Graph Convolutional Network](https://tkipf.github.io/graph-convolutional-networks/)\n",
    "\n",
    "Graph Neural Networks are semi-supervised learning method that applies neural networks to learn a graph representation of a real-world problem\n",
    "### GNN Workflow\n",
    "\n",
    "1. Graph representation of a real-world problem\n",
    "2. GNN modelling \n",
    "\n",
    "### Graph Structure\n",
    "* Graphs \\\n",
    "Data is modelled as graphs\n",
    "$$G = (V, E)$$\n",
    "\n",
    "* Vertices \\\n",
    "Each node contains a vector representation of an object \n",
    "\n",
    "* Edges \\\n",
    "Each edge contains the relationship between its vertices\n",
    "\n",
    "* Modelling representations\n",
    "    - Each feature $x_i$ for every node $i$ is summarized in a $N \\times D$ feature matrix $D$ ($N$ : number of nodes, $D$ : number of input features)\n",
    "    - The graph sturcture is typically represented in an $N \\times N$ adjacency matrix $A$, where each value represents the edges \n",
    "\n",
    "\n",
    "### General Learning Process\n",
    "Define learning time steps (eg. $s$)\n",
    "\n",
    "* Node updating function\n",
    "$${h}_{t}^{n} = q {(}{{h}^{n}_{t-1}}, \\underset{\\forall n_{j}:n \\stackrel{k}\\rightarrow n_j}{\\bigcup}f_{t}({h}^{n}_{t-1},{h}^{n_j}_{t-1}) {)} $$\n",
    "\n",
    "* Message function\n",
    "$$\\underset{\\forall n_{j}:n \\stackrel{k}\\rightarrow n_j}{\\bigcup}f_{t}({h}^{n}_{t-1},{h}^{n_j}_{t-1})$$\n",
    "\n",
    "eg. dimensionality reduction\n",
    "\n",
    "The idea of updating is called \"passing on the message\"\n",
    "\n",
    "* Unsupervised Process\n",
    "\n",
    "Imagine a clock,\n",
    "\n",
    "For every tick, each node updates its value from the neighbors\n",
    "\n",
    "There is no notion of convergence. How many times you wanna update each node is hyperparameter. (a.k.a. n-order neighborhood information)\n",
    "\n",
    "With an adequate number of \"message passing\", even a randomly initialized weights can properly learn the relationship between the nodes, and a latent representation of the graph\n",
    "\n",
    "* Supervised Process\n",
    "\n",
    "The latent representation produced at the end of the unsupervised process can be used for downstream tasks such as\n",
    "\n",
    "1. node selection : utilize the learned vector\n",
    "\n",
    "2. node classification : with a given label, conduct supervised learning\n",
    "\n",
    "3. graph classification : summarize all the vectors in the graph\n",
    "\n",
    "4. etc.\n",
    "\n",
    "With these tasks, we can optimize the weights for our specific tasks\n",
    "\n",
    "#### Modelling problems\n",
    "\n",
    "1. prepare the messages (f function)\n",
    "\n",
    "2. summarize the received information (Union logic)\n",
    "\n",
    "3. update the node state (q function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN for text classification\n",
    "- [Paper](https://arxiv.org/pdf/1809.05679.pdf)\n",
    "- [github](https://github.com/yao8839836/text_gcn)\n",
    "\n",
    "##### Pooling Function \n",
    "$$ L^{(j+1)} = p(\\hat{A}L^{(j)}W_{j})$$\n",
    "\n",
    "##### Representation\n",
    "  * $(V + D) \\times (V + D)$ Adjacency matrix where $V$ = number of vocabularies and $D$ = number of documents\n",
    "  * ${PMI}(i,j)$ where   $i,j$ are words ${PMI}(i,j)>0$ \n",
    "  * ${TF-IDF}_{i,j}$ where $i$ is document, $j$ is word\n",
    "  * $1$ where $i = j$\n",
    "  * $0$ otherwise\n",
    "\n",
    "##### Structure\n",
    "  - Two layers of GCN with ReLU : second layer node (word + document) embeddings have the same size as the labels set + softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Attention Network (GAT)\n",
    "- [Paper](https://arxiv.org/pdf/1710.10903.pdf)\n",
    "- [github](https://github.com/PetarV-/GAT)"
   ]
  }
 ]
}